---
layout: post
title:  "[논문 review] AdaIN"
image: ''
date:   2019-02-24 19:35:00
tags:
- normalization, AdaIN, CNN, BN
description: 'AdaIN'
categories:
- Normalization
---

<br/><br/>
번역 + 요약 + 이해를 종합해서 써놓은 글입니다. 질문 + 피드백 환영합니다 :)

## Abstract


<p class="music-read"><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.pdf">논문 URL</a></p>
<br/>
2017년 3월, style transfer(Gatys et al.)이 발표된 후에 발표된 논문이다. 당시 stlye transfer는 뛰어난 결과로 학계를 넘어서
적잖은 센세이션을 불러일으켰다. 이 논문은 style transfer neural networks의 limitation을 문제 삼은 논문이다.
style transfer은 훈련과정이 반복적인 최적화 과정으로 이루어져있어 매우 느리다. 이를 해결하기 위한 feed-forward neural network가 제안되었지만,
그에 따른 비용이 따랐다: 정해져 있는 style로만 transfer가 가능했다. 이 논문에서 이를 해결하기 위해 간단하지만 효과적인 **AdaIN (Adaptive Normalization)**을
제안한다.
<br/>
## Introduction
style transfer 논문이 시사한 바는 2가지이다. DNN이 content 뿐만 아니라 style 정보도 가지고 있다는 점, 그리고 content와 style이 분리 가능하다는 점이다.
굉장히 flexible하게 적용이 가능하지만, optimization process가 매우 느리다.
<br/>
이를 해결하기 위한 많은 시도가 있었다. Abstract에서도 언급했듯이, 그 중 대표적인 feed-forward 방식은 한가지 style에만 국한된다는 한계가 있다.
<br/>
이 지긋지긋한 속도 문제를 우리는 아주 간단하게 해결하였다. feed-forward와 대등한 속도로 훈련이 가능하며, 
optimization-based framework (원래의 방법)과 대등한 flexibility를 지니고 있고, 임의의 style을 real-time으로 transfer할 수 있다.
<br/>
기존에 IN (Instance Normalization) 개념이 이미 발표된 바 있고, 이를 응용하였다. (feed-forward 방식에서 놀라울정도로 effective 했다고..)
<br/>

## Background
1. BN
<br/>
<a href= https://dlehgo14.github.io/Batch-Normalization/>BN</a>은 mini-batch 별로 진행하는 normalization 방식으로, RNN에 적용하기 위한 몇가지 추가적인 방법이 제안되었다.
<br/><br/>
2. IN
<br/>
기존의 feed-forward stylization 방법에서 conv 후에 BN layer을 시행했다. 그러나 BN을 IN을 바꾸니 놀랍게도 성능이 향상했다.
<img src="/assets/img/AdaIN/1.png">
<br/><br/>
BN과 다른점은 µ(x)와 σ(x)가 각각의 spatial independent하게 각각의 channel 별로 계산된다는 것이다.
<br/>
또 하나의 다른점은 IN은 BN과 달리 test 시에 mean과 variance를 population statics로 사용하지 않는다는 것이다.
<br/><br/>
3. Conditional Instance Normalization
<br/>
CIN의 핵심은 affine parameters(γ, β)를 다양하게 사용하는 것이다. 
<img src="/assets/img/AdaIN/2.png">
<br/>
각각의 스타일마다 다른 s 값을 가지고 있고, (논문에서는 32개) 각각 해당하는 s 개의 style로 transfer할 수 있게된다.
<br/>
하지만 parameter 수가 증가하여 훈련이 느리고, s개 의외의 새로운 style을 적용시키는 것이 불가능하다는 한계점이 있다.
<br/><br/>
4. Interpreting Instance Normalization
<br/>
CIN이 굉장히 잘 되었지만, 어떻게 특정 style을 그렇게 따로따로 잘 훈련시킬 수 있는지는 명확하지 않았다. 
<br/>
아마도 IN은 style normalization을 수행한다고 추정할 뿐이다. 
<br/><br/>
5. Adaptive Instance Normalization
<br/>
CIN에서 특정 affine parameter에 따라 특정 style로의 transfer가 가능했다. 그렇다면 arbitrary affine parameters를 통해 임의의 style로의 변환도 가능하지 않을까?
<br/>
이런 아이디어에서 탄생한 개념이 이 논문에서 제안한 AdaIN: Adaptive Instance Normalization이다. 
<br/>
BN, IN, CIN과 다르게 AdaIN은 **affine parameters를 훈련하지 않는다.** content input x와 style input y를 받아서, y를 이용해 affine parameter를 계산한다.
<br/>
<img src="/assets/img/AdaIN/3.png">
<br/>
y의 mean과 variance도 IN과 마찬가지로 channel-wise 방식으로 계산된다.

<br/><br/>
## Experimental Setup
<br/>
<img src="/assets/img/AdaIN/4.png">
*overview of overall network*

<br/>
### Architecture
<br/>
input으로 content image **c**와 style image **s**를 받는다. 간단한 encoder-decoder 구조로 이루어져 있다.
<br/>
> t = AdaIN ( f(c), f(s) )

<br/>
f(c), f(s)는 encoder에 의해 압축된 latent vector이고, t는 AdaIN의 output이다.

> T(c, s) = g(t)

<br/>
t 가 encoder(g)를 통해 stylized image를 output으로 내놓는다.
<br/>
encoder의 처음 몇 레이어는 pretrained vgg-19 네트워크를 고정하여 사용하였고, decoder는 encoder를 mirror 하였다. (pooling layer을 
upsampling으로 대체하여서)
<br/>
최대한 네트워크를 간단하게 구성하여 다른 변인을 최소화하려는 목적으로 해석된다.
<br/><br/>
### Training
<br/>
MS-COCO 데이터셋을 content images로 사용하였고, WikiArt에서 대부분 수집된 painting을 style image로 사용하였다.
<br/>
각각의 데이터 셋은 약 80,000장씩으로 구성되어 있다. Adam optimizer를 사용하였고, batch size는 8로 하였다.
<br/>
Loss는 content loss와 stlye loss에 style loss weight를 곱한 값의 합으로 하였다.
<br/>
> L = Lc + λ*Ls

<br/>
Content loss의 target을 특이하게 AdaIN의 output t로 하였다. 네트워크의 아웃풋 (g(t))를 다시 인코더에 넣어 뽑은 feauture를 t와 비교하였다.
<br/>
> Lc = || f(g(t)) − t ||(2) 

<br/>
Style loss는 IN statics를 이용하여 계산하였다. (Gram matrix loss도 비슷한 결과를 냈지만, 개념적으로 깔끔하게 하기 위해 다음 loss를 사용하였다.)
<br/>
<img src="/assets/img/AdaIN/5.png">
<br/>
φ 는 VGG-19를 이용한 style loss를 말하고, 해당 논문에서 relu_1, relu2_1, relu3_1, relu4_1을 사용하였다.
<br/><br/>

## Results
<br/>
style transfer에서 3-way tade off가 있다. **speed, flexibility, 그리고 quality**이다.
<br/>
**질적인 측면에서,** 이 논문의 결과는 상당히 괜찮았다. Ulyanov의 method와는 다르게 이 논문의 method는 특정 style을 train 시에 미리 노출시키지 않은 점을 고려하여야 한다. 그리고 Gatys의 method는 optimization process의 speed가 굉장히 오래 걸리는 점도 감안하여야 한다.
<br/>
즉, 다른 방법들은 특정 style을 훈련시킨 것이거나 훈련에 오랜 시간이 걸린다. 이 논문은 general한 style로 모두 적용이 가능하도록 훈련됐으면서(flexibility), optimization process도 오랜시간이 걸리지 않는다.(speed) (test 시에 사용된 style은 training 시에 이용되지 않았다.)
<img src="/assets/img/AdaIN/6.png">
<br/>
**양적인 측면에서**도 상당히 괜찮은 결과를 보였다. loss가 다른 method에 비해 높지만, speed와 flexiblity를 확보한 것을 감안하여야 한다.
<img src="/assets/img/AdaIN/7.png">
<br/>
다음은 적용 가능한 stlye 수와 이미지 사이즈에 따라 요구되는 훈련시간이다.
<img src="/assets/img/AdaIN/8.png">
<br/>
## Runtime result
<br/>
추가적으로, runtime에 동적으로 style의 반영 정도를 조절하는 것이 가능하다. 
> T(c, s, α) = g((1 − α)f(c) + αAdaIN(f(c), f(s)))

<img src="/assets/img/AdaIN/9.png">

<br/>
이 외에도 스타일을 합성 (style interpolation)하거나 spatial and color control이 가능하다.
<img src="/assets/img/AdaIN/10.png"><br/>
<img src="/assets/img/AdaIN/11.png">
<br/>
## 총평 및 느낀점
많은 특성들은 trade-off 속에서 조정이 가능하다. 저자는 style transfer network가 style 특성을 인식하는 원리를 깊이 고민했을 것 같다. 그리고 얻은 이해를 통해 quality의 약간의 희생을 통한 speed와 flexibility에 이점을 얻는 네트워크를 고안해 낼 수 있었다. 이러한 심오한 이해를 통해 AdaIN과 같은 아이디어가 탄생하는 것 같다.
<br/>
이것은 정말 부차적인 것이지만, fully-conv network의 이점 중 하나는 어떠한 사진크기에도 적용이 가능하다는 점이 있다.
<br/><br/>
*정신 없이 쓰느라 두서 없고 이해도 어려운 글일 것 같네요. 틀린 내용이나 보완할 내용이 있다면 댓글로 부탁드리겠습니다!*