---
layout: post
title:  "Andrew Ng 머신러닝 강의 정리"
image: ''
date:   2019-02-26 15:00:00
tags:
- ML
description: 'machine learning'
categories:
- Andrew Ng 머신러닝
---

<a href="https://www.youtube.com/watch?v=PPLop4L2eGk&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN">Andrew Ng 박사님의 강의</a>를 보고 정리한 글입니다.
<br><br>
## Lec1.1

<br>
ML -> give computers **ability to learn without explicilty programmed.** (Arthur Samuel, 1959)
<br>
ML -> E (experience), T (task), P (performance meausure) **performance on T, as measured by P, improved with E** (Tom Mitchell, 1998)
<br>
Can you distinguish which is E or T or P?<br>

> the program that marks spam mails among total mails

<br>
T: Classifying emails as spam or not spam
<br>
P: The number of emails correctly classified
<br>
E: Watching you label emails as spam or not
<br><br>

## Lec2.2

### supervised learning
<br>
Given right answers -> **Regression** -> predict continuous output
<br>
Given right answers -> **Classification** -> predict 0 or 1

## Lec3.3

### Unsupervised learning
<br>
There is no label, so the algorithm should discover the features as well as which belongs to the features.
<br><br>
## Lec4.4
<br>
cost 가 점점 증가한다면 learning rate를 줄여야한다. Over shooting 되고 있는 상황일 것이기 때문이다.
<br>
cost 가 증가하다 감소하다 증가하다 감소하다를 반복해도 learning rate를 줄여야한다.
<br>
바람직한 상황은 cost가 점점 줄어들고, 아주 작아져서는 천천히 converge하는 것이다.