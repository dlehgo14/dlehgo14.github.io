---
layout: post
title:  "[Lecture] Data Mining_1"
image: ''
date:   2019-03-15 15:09:27
tags:
- Data mining
description: 'Data Mining'
categories:
- Lectures
---
This post is based on the lecture "Data mining" of professor Cho Sung Bae from Yonsei univ.

---

`03.04 Monday`

Data gets bigger and bigger. From **37.4 exabytes** in  2013, to **125 exabytes** in 2017. Its growing rate is **42.5%** per year. *(in case of unstructured data)*

>  Byte --> Kilobyte --> Megabyte --> Terabyte --> Petabyte --> Exabyte --> Zettabyte --> Yottabyte

Then when these **big data** can be used?

- Manufacturing
  analysis can be used in **prognostics** or **supply planning**.
- Health care
  providing **personilzed medicine**.
- Education
  improved learning programs.
- Media & entertainment
  understanding preferences.
- Social network
  recommending, grouping (of people)
- ETC
  Finance, insurance, IoT, E-commerce, Bioinformatics

The growth of data is explosive. Then how can we treat it? By **automating the collection and availability** of data. 
Where these explosive big data comes from? It is from **business** and **science** and **society, everyone**.
What are the **definition** of big data? [**3Vs**](https://whatis.techtarget.com/definition/3Vs)

1. Volume
2. Velocity
3. Variety

![image of 3Vs](https://itknowledgeexchange.techtarget.com/writing-for-business/files/2013/02/BigData.001.jpg "What is 3Vs?")

~~Veracity: Data should be generated by organic distributed process.~~ 

### Data deluge

The quantity of information is soaring. So someone has to make the information meaningful. This process is [data deluge](https://www.economist.com/leaders/2010/02/25/the-data-deluge "The economists").

 

### Inductive Inference

It is the most important intellectual activity of humans. We know the snowy road is slippery by experience.
What are the factors that influence the accuracy of induction?

- amount of past data
- quality of past data

So it is important to deluge **interesting data** from soaring data. This is **Data mining**. And other names of it are advanced analytics, predictive analytics, machine learning, and pattern recognition. 

> Interesting data which is **non-trivial, implicit, previously unknown,** and **potentially useful**.

---

`03.11 Monday`

### Business Analytics

It is **predicting new records** based on past data.

### Data Mining

which copes with **huge amounts of data**. Let's go deeper into the data mining.



## Supervised Learning

### Classification

Predict the **categorical target** which is **binary**.

### Prediction

Predict **numerical target**

> Both **classification** and **prediction** are called **predictive analytics**.



## Unsupervised Learning

Goal : **What goes with what**, which is **Segment** 
Also called in **affinity analysis**, or **market basket analysis**.



## Reduction

### Dimension reduction

Larger the complexity of models, Bigger amount of data which is required *exponentially*.
This is called **curse of dimensionality**.
So reduce the number of variables (columns or attributes).

### Data reduction

Reduce the number of records (rows), by clustering or others.
Why this is needed? Because some datum can be so similar, so it is not useful, or even may causes **skew**.
Then how does it?

### Data visualization

By using graphs or plots of data, it is useful to find the relationships between paris of variables.

### Data exploration

Understand global meaning of the data, and detect unusual values *by visualization and reduction*.
**Reviewing** the data to help data mining task.

---



### Steps in data mining

There are 10 steps for data mining.

1. Define/ understand purpose.
2. <u>Obtain data.</u>
3. Explore, clean, pre-process data
4. Reduce the data dimensions, if necessary.
5. <u>Determine task (classification, prediction, etc)</u>
6. Partition data *(__given values__, and __values to predict__ for supervised task)*
7. Choose the techniques *(regression, CART, dnn, etc)*
8. Iterative implementaion and tuning
9. <u>Evaluate results *(compare models)*</u>
10. Deploy

The most important steps are **2, 5**, and **9**.

### Obtain data

typically use **sampling** to produce statistically-valid results from huge data.
Often, our interesting events are rare. So there are **too few interesting cases** in the data set, so have to **oversample** them to obtain a balanced training set.

#### variables

categorical (orderd or non-ordered) or numerical
More is not necessary **(becuase need more records: __curse of dimensionality__)*

> Rules of thumbs:
>
> 10 records per variable
>
> 6 * (#classess) * (#variables) records

**Domain knowledge** is also required as well as visualization and dimension reduction

#### detecting outliers

Outliers may cause disproportionate influence on models, so should be detected in pre-processing.
In some contexts, detecing outliers can be the purpose of DM. *(Anomaly detection)*

#### Missing values

There can be some missing values in data set. Default is **to drop thoses records**. There are some other solutions

1. Omission

   If small records have missing values, **omit those records**.
   If values are missed only in small set of variables, **omit those variables**.
   If many records have missing values, **ommision is not practical**.

2. Imputation

   Replace missing values with reasonable substitutes.
   How? Keep records of non-missing values, and use them.

---

`03.13 Wednesday`

#### Normalizing

will make the algorithm effective.

#### Overfitting

Why overfitting occurs?

- Too many predictors (attributes)
- Too many parameters
- Trying many different models *(choose best one, but it can be overfitted one.)*

#### Partitioning data

training, validation, test

to prohibit overfitting

If data is too small, use **cross-validation**.

<br>

{% include lectures/datamining.html %}